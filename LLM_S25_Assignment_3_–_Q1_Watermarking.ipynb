{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patrick59zm/BLEND/blob/master/LLM_S25_Assignment_3_%E2%80%93_Q1_Watermarking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-Yh9MQXTDRmM",
        "outputId": "224577fb-0e3c-474a-f7fc-0cdc22ad41ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9d7864370ad9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogitsProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1965\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0malbert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/layoutlmv2/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"__file__\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LazyModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefine_import_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m__spec__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36mdefine_import_structure\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m   2389\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0mdefined\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mfrozensets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdicts\u001b[0m \u001b[0mof\u001b[0m \u001b[0mstrings\u001b[0m \u001b[0mto\u001b[0m \u001b[0msets\u001b[0m \u001b[0mof\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m     \"\"\"\n\u001b[0;32m-> 2391\u001b[0;31m     \u001b[0mimport_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_import_structure_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2392\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspread_import_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimport_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36mcreate_import_structure_from_path\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2162\u001b[0;31m             \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2164\u001b[0m         \u001b[0;31m# Remove the .py suffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Install Dependencies and Download Models\n",
        "\n",
        "!uv pip install -q transformers datasets --prerelease disallow\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import LogitsProcessor, set_seed\n",
        "import numpy as np\n",
        "import datasets\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda:0'\n",
        "model.to(device)\n",
        "model = model.eval()"
      ],
      "metadata": {
        "id": "tz7QhdIWEKI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download lab files\n",
        "\n",
        "import sys\n",
        "\n",
        "!rm -rf llm_lab\n",
        "\n",
        "![ ! -d 'llm_lab' ] && git clone https://github.com/ethz-privsec/llm_lab.git\n",
        "%cd llm_lab\n",
        "!git pull https://github.com/ethz-privsec/llm_lab.git\n",
        "%cd ..\n",
        "if \"llm_lab\" not in sys.path:\n",
        "  sys.path.append(\"llm_lab\")"
      ],
      "metadata": {
        "id": "mPjncmYxMVEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example of how to generate 100 tokens of text without watermarking\n",
        "\n",
        "from llm_lab.gpt_generate import generate_with_seed, gen_red_list\n",
        "\n",
        "prompt = \"Boston is one of the oldest municipalities in America,\"\n",
        "print(generate_with_seed(model, tokenizer, prompt, seed=42))"
      ],
      "metadata": {
        "id": "dox-UMAjMdPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will now implement three different watermarking schemes:\n",
        "1. A simple scheme that never outputs the letter 'e' (lowercase or uppercase)\n",
        "2. A red-list scheme, that generates a random list of banned tokens for each token generation.\n",
        "3. A soft red-list scheme, that also generates a random red-list, but just biases the LLM against these tokens instead of outright banning them, by substracting the value `logit_offset=2` from the logits of each red-listed token.\n",
        "\n",
        "You should implement each of these schemes as a `LogitsProcessor` class.\n",
        "\n",
        "For the red-list schemes, you should use `gpt_generate.gen_red_list` to generate a red list containing 50% of the LLM's vocabulary.\n",
        "The seed for generating the pseudorandom red list is computed from the previous token processed by the model.\n",
        "\n",
        "So for example, if the model has so far processed the string \"my name is \" (which tokenizes as `[1820, 1438, 318, 220]`), then the red list for the next token to be generated is `**gen_red_list(torch.LongTensor([220]), model.config.vocab_size)** = [43383,  7006, 40846, ...]`."
      ],
      "metadata": {
        "id": "381ySX2dOPip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Exercise 1: Implement a trivial watermarking scheme that samples text without any 'e' (lowercase or uppercase)\n",
        "\n",
        "class NoEsLogitsProcessor(LogitsProcessor):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def __call__(self, input_ids, scores):\n",
        "        \"\"\"\n",
        "        Processes the output scores of the LLM before generating the next token.\n",
        "        Args:\n",
        "            input_ids: torch.LongTensor of shape (batch_size, sequence_length) — Indices of input sequence tokens in the vocabulary.\n",
        "            scores: torch.FloatTensor of shape (batch_size, model.config.vocab_size) — Logits for the next token to be generated.\n",
        "        Returns: torch.FloatTensor of shape (batch_size, model.config.vocab_size) — The processed logits.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "        return scores\n",
        "\n",
        "\n",
        "no_e_processor = NoEsLogitsProcessor()\n",
        "\n",
        "prompt = \"Anton Vowl is missing. Ransacking his Paris flat, a group of his faithful companions trawl through his diary for any hint as to his location and, insidiously, a ghost, from Vowl's past starts to cast its malignant shadow.\\n \"\n",
        "output = generate_with_seed(model, tokenizer, prompt, logits_processor=no_e_processor, seed=42)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "SZN3-Y9wMxF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Exercise 2: Implement a red-list watermarking scheme\n",
        "\n",
        "class RedListLogitsProcessor(LogitsProcessor):\n",
        "    def __init__(self, red_frac=0.5, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.red_frac = red_frac\n",
        "\n",
        "    def __call__(self, input_ids, scores):\n",
        "        \"\"\"\n",
        "        Processes the output scores of the LLM before generating the next token.\n",
        "        Args:\n",
        "            input_ids: torch.LongTensor of shape (batch_size, sequence_length) — Indices of input sequence tokens in the vocabulary.\n",
        "            scores: torch.FloatTensor of shape (batch_size, model.config.vocab_size) — Logits for the next token to be generated.\n",
        "        Returns: torch.FloatTensor of shape (batch_size, model.config.vocab_size) — The processed logits.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "        return scores\n",
        "\n",
        "red_list_processor = RedListLogitsProcessor()\n",
        "\n",
        "prompt = \"Boston is one of the oldest municipalities in America,\"\n",
        "output = generate_with_seed(model, tokenizer, prompt, logits_processor=red_list_processor, seed=42)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "3hk2kd6MNSfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Exercise 3: Implement a soft red-list watermarking scheme\n",
        "\n",
        "class SoftRedListLogitsProcessor(LogitsProcessor):\n",
        "    def __init__(self, red_frac=0.5, logit_offset=2.0, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.red_frac = red_frac\n",
        "        self.logit_offset = logit_offset\n",
        "\n",
        "    def __call__(self, input_ids, scores):\n",
        "        \"\"\"\n",
        "        Processes the output scores of the LLM before generating the next token.\n",
        "        Args:\n",
        "            input_ids: torch.LongTensor of shape (batch_size, sequence_length) — Indices of input sequence tokens in the vocabulary.\n",
        "            scores: torch.FloatTensor of shape (batch_size, model.config.vocab_size) — Logits for the next token to be generated.\n",
        "        Returns: torch.FloatTensor of shape (batch_size, model.config.vocab_size) — The processed logits.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "        return scores\n",
        "\n",
        "soft_red_list_processor = SoftRedListLogitsProcessor()\n",
        "\n",
        "prompt = \"Boston is one of the oldest municipalities in America,\"\n",
        "output = generate_with_seed(model, tokenizer, prompt, logits_processor=soft_red_list_processor, seed=42)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "S72scoD3Rhi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay! We're now ready to start generating watermarked text.\n",
        "We give you 20 prompts in `data/watermark_prompts.txt`.\n",
        "For each of these, generate 100 more tokens using each of the three watermarking schemes, and save all of this as a numpy array for submission.\n",
        "\n",
        "MAKE SURE TO USE `seed=42` FOR ALL YOUR GENERATIONS."
      ],
      "metadata": {
        "id": "SE72pkVOR1hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import trange\n",
        "\n",
        "with open('llm_lab/data/watermark_prompts.txt') as f:\n",
        "  prompts = f.read().splitlines()\n",
        "\n",
        "processors = [no_e_processor, red_list_processor, soft_red_list_processor]\n",
        "outputs = []\n",
        "\n",
        "seed = 42  # DON'T CHANGE THIS!!!\n",
        "\n",
        "for i in trange(20):\n",
        "  min_new_tokens = 100\n",
        "  max_new_tokens = min_new_tokens\n",
        "\n",
        "  for j in range(3):\n",
        "    output = generate_with_seed(model, tokenizer, prompts[i], logits_processor=processors[j],\n",
        "                                min_new_tokens=min_new_tokens, max_new_tokens=max_new_tokens, seed=seed)\n",
        "    outputs.append(output)\n",
        "\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "8TSmCjyPSVPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the final part of this question, you now have to try and detect watermarked text.\n",
        "We give you 80 pieces of text in `data/watermarked_gens.npy`.\n",
        "For each piece of text you have to guess whether it was generated with:\n",
        "\n",
        "1.   No watermark\n",
        "2.   The dummy \"no E's\" watermark\n",
        "3.   The red-list watermark\n",
        "4.   The soft red-list watermark\n",
        "\n",
        "We use the same `generate_with_seed` and `gen_red_list` implementations as you. Our red-list watermarking scheme also uses the same parameters (i.e., 50% of the tokens are red-listed, and for the soft version we substract 2.0 from the logits).\n",
        "\n",
        "Exactly 20 of the 80 texts are generated with each of the 4 options above. Each text is comprised of a short prompt, followed by 100-200 generated tokens.\n",
        "\n",
        "Store your guesses (1,2,3,4) for each piece of text in a numpy array."
      ],
      "metadata": {
        "id": "YavEBmQ-TRcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs_secret = np.load(\"llm_lab/data/watermarked_gens.npy\", allow_pickle=True)\n",
        "assert len(outputs_secret) == 80\n",
        "\n",
        "my_guesses = [1] * 20 + [2] * 20 + [3] * 20 + [4] * 20"
      ],
      "metadata": {
        "id": "s3J4KW7gUZwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export your solution"
      ],
      "metadata": {
        "id": "0HubQ15ar2Ng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save your results, you can use the code below, which will save the file in Colab's temporary storage (or locally, if you're not using Colab), or on your Google Drive. If you save it on Colab's temporary storage, you can download it from there (see the file system icon on the left)."
      ],
      "metadata": {
        "id": "Agl7Jh9h0zWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llm_lab.utils import get_solution_path, is_valid_student_id\n",
        "\n",
        "#@markdown Check this box if you want to save your results on Google Drive. Otherwise they'll be\n",
        "#@markdown saved on the ephimeral Colab storage. The storage will be deleted with the runtime,\n",
        "#@markdown so REMEMBER TO DOWNLOAD THE FILES before you close the tab!\n",
        "SAVE_ON_DRIVE = True # @param {\"type\":\"boolean\"}\n",
        "\n",
        "#@markdown The number on your Legi (Student ID card). It's in the format 'dd-ddd-ddd'\n",
        "STUDENT_ID = \"00-000-000\"  # @param {\"type\":\"string\",\"placeholder\":\"00-000-000\"}\n",
        "\n",
        "assert is_valid_student_id(STUDENT_ID), \"Student ID should have the format 'dd-ddd-ddd'\"\n",
        "\n",
        "SOLUTIONS_PATH = get_solution_path(STUDENT_ID, SAVE_ON_DRIVE)"
      ],
      "metadata": {
        "id": "BOLS_NA8O0Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save generations\n",
        "assert len(outputs) == 60\n",
        "np.save(SOLUTIONS_PATH / \"Q1_gens.npy\", np.asarray(outputs))\n",
        "\n",
        "# Save guesses\n",
        "assert len(my_guesses) == 80\n",
        "np.save(SOLUTIONS_PATH / \"Q1_guesses.npy\", np.asarray(my_guesses))"
      ],
      "metadata": {
        "id": "eTuZ9Y0Kr5Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DocvhU1UDKJk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}